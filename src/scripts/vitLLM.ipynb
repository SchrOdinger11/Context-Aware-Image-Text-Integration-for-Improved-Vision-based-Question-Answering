{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open image from a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageFromJson(json_file_path, images_folder):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        image_map = json.load(file)\n",
    "\n",
    "    # Iterate over the items in the JSON file\n",
    "    for image_name, question in image_map.items():\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(images_folder, image_name)\n",
    "\n",
    "        try:\n",
    "            # Open the image\n",
    "            with Image.open(image_path) as img:\n",
    "                img.show()  # This will display the image using the default image viewer\n",
    "                print(f\"Opened {image_name} with question: {question}\")\n",
    "        except IOError:\n",
    "            # Handle scenarios where the image cannot be opened\n",
    "            print(f\"Failed to open image {image_name} at {image_path}\")\n",
    "        \n",
    "\n",
    "# json_file_path = '../dataset/question_image_mapping.json'\n",
    "# images_folder = './segmentedImage'\n",
    "# getImageFromJson(json_file_path, images_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin QAVIT here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/vqa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5Tokenizer, T5EncoderModel, ViTModel, T5ForConditionalGeneration\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import get_cosine_schedule_with_warmup, AdamW\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA-VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAViT(nn.Module):\n",
    "    def __init__(self, vision_model):\n",
    "        super(QAViT, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        visual_outputs = self.vision_model(pixel_values=image)\n",
    "        visual_features = visual_outputs.last_hidden_state\n",
    "        print(\"hola amigo\",visual_features.shape)\n",
    "        #visual_outputs.last_hidden_state = visual_features\n",
    "        return visual_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I have json which maps question_id to the segmentedImage. Now I will add this information of segmentedImage in the combined json file\n",
    "# and finally my dataset will be annotaion[question] , annotation [image_file], annotation [multiple_choice_answer]\n",
    "class QADataset():\n",
    "    def __init__(self, image_dir, questionId_image_path, annotations, tokenizer, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.annotations= annotations\n",
    "        self.transform = transform #or transforms.ToTensor()  # Default transform\n",
    "\n",
    "        # Load the question-image mapping\n",
    "        with open(questionId_image_path, 'r') as f:\n",
    "            print(\"boyy\")\n",
    "            questionId_image_map = json.load(f)\n",
    "\n",
    "\n",
    "# Reversing keys and values\n",
    "        self.reversed_dict = {value: key for key, value in questionId_image_map.items()}\n",
    "        # Load the combined data JSON\n",
    "        # with open(combined_data_path, 'r') as f:\n",
    "        #     print(\"hood\")\n",
    "        #     combined_data = json.load(f)\n",
    "        \n",
    "        # print(combined_data)\n",
    "        # # Merge the data\n",
    "        # self.annotations = []\n",
    "        # for data in self.combined_data:\n",
    "        #     question_id = data[\"question_id\"]\n",
    "        #     # Match image file with the question_id from the question_image_map\n",
    "        #     if str(question_id) in self.question_image_map:\n",
    "        #         image_file = self.questionId_image_map[str(question_id)]\n",
    "        #         data[\"image_file\"] = image_file  # Add image file name to the data\n",
    "        #         self.annotations.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        question_id = annotation[\"question_id\"]\n",
    "        image_file = self.reversed_dict[question_id]\n",
    "        image_path = os.path.join(self.image_dir, image_file)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        question = annotation[\"question\"]\n",
    "        answer = annotation[\"multiple_choice_answer\"]\n",
    "        question_tokens = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"question_tokens\": {k: v.squeeze(0) for k, v in question_tokens.items()},\n",
    "            \"answer\": answer\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_collate_fn(batch):\n",
    "    # Stack images\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "\n",
    "    # Prepare questions\n",
    "    question_tokens = {k: torch.cat([item[\"question_tokens\"][k] for item in batch]) for k in batch[0][\"question_tokens\"]}\n",
    "    padded_question_tokens = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence([item[\"question_tokens\"][\"input_ids\"] for item in batch], batch_first=True, padding_value=0),\n",
    "        \"attention_mask\": torch.nn.utils.rnn.pad_sequence([item[\"question_tokens\"][\"attention_mask\"] for item in batch], batch_first=True, padding_value=0),\n",
    "    }\n",
    "\n",
    "    # Tokenize and pad answers\n",
    "    answers = [item[\"answer\"] for item in batch]\n",
    "    answer_tokens = tokenizer(answers, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        \"question_tokens\": padded_question_tokens,\n",
    "        \"answer\": answer_tokens\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(image_dir, questionId_image_path,json_path, tokenizer, transform):\n",
    "    # Load the annotations from JSON\n",
    "    #json_path is combined json\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    annotations = data[\"annotations\"]\n",
    "\n",
    "    # Split into train (80%), val (10%), and test (10%)\n",
    "    train_annotations, test_annotations = train_test_split(annotations, test_size=0.2, random_state=42)\n",
    "    val_annotations, test_annotations = train_test_split(test_annotations, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Create Dataset objects\n",
    "\n",
    "    train_dataset = QADataset(image_dir,questionId_image_path, train_annotations, tokenizer, transform)\n",
    "    val_dataset = QADataset(image_dir,questionId_image_path, val_annotations, tokenizer, transform)\n",
    "    test_dataset = QADataset(image_dir,questionId_image_path, test_annotations, tokenizer, transform)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_t5(t5_model):\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q\", \"k\"]\n",
    "    )\n",
    "    return get_peft_model(t5_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qavit_with_validation(qavit_model, t5_model, train_loader, val_loader, tokenizer, num_epochs, device):\n",
    "    qavit_model.train()\n",
    "    t5_model.train()\n",
    "\n",
    "    # Apply LoRa to T5 model\n",
    "    t5_model = apply_lora_to_t5(t5_model)\n",
    "\n",
    "    # Optimizer and Scheduler\n",
    "    optimizer = AdamW([\n",
    "        {\"params\": qavit_model.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": t5_model.parameters(), \"lr\": 5e-5}\n",
    "    ])\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=1000,\n",
    "        num_training_steps=len(train_loader) * num_epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        total_loss = 0\n",
    "        qavit_model.train()\n",
    "        t5_model.train()\n",
    "        for batch in train_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            question_tokens = {k: v.to(device) for k, v in batch[\"question_tokens\"].items()}\n",
    "            answers = {k: v.to(device) for k, v in batch[\"answer\"].items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through QA-ViT model\n",
    "            visual_outputs = qavit_model(images)\n",
    "            visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # Forward pass through T5 model\n",
    "            encoder_outputs = (visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1), None, None)\n",
    "\n",
    "            t5_outputs = t5_model(\n",
    "                input_ids=question_tokens[\"input_ids\"],\n",
    "                attention_mask=question_tokens[\"attention_mask\"],\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                labels=answers[\"input_ids\"]\n",
    "            )\n",
    "\n",
    "            loss = t5_outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        qavit_model.eval()\n",
    "        t5_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                question_tokens = {k: v.to(device) for k, v in batch[\"question_tokens\"].items()}\n",
    "                answers = {k: v.to(device) for k, v in batch[\"answer\"].items()}\n",
    "\n",
    "                # Forward pass through QA-ViT model\n",
    "                visual_outputs = qavit_model(images)\n",
    "                visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "                # Forward pass through T5 model\n",
    "                encoder_outputs = (visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1), None, None)\n",
    "\n",
    "                t5_outputs = t5_model(\n",
    "                    input_ids=question_tokens[\"input_ids\"],\n",
    "                    attention_mask=question_tokens[\"attention_mask\"],\n",
    "                    encoder_outputs=encoder_outputs,\n",
    "                    labels=answers[\"input_ids\"]\n",
    "                )\n",
    "\n",
    "                loss = t5_outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boyy\n",
      "boyy\n",
      "boyy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/vqa/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola amigo torch.Size([8, 197, 768])\n",
      "Epoch 1/5, Training Loss: 40.3262\n",
      "hola amigo torch.Size([1, 197, 768])\n",
      "Epoch 1/5, Validation Loss: 7.6297\n",
      "hola amigo torch.Size([8, 197, 768])\n",
      "Epoch 2/5, Training Loss: 39.4640\n",
      "hola amigo torch.Size([1, 197, 768])\n",
      "Epoch 2/5, Validation Loss: 7.5847\n",
      "hola amigo torch.Size([8, 197, 768])\n",
      "Epoch 3/5, Training Loss: 40.7731\n",
      "hola amigo torch.Size([1, 197, 768])\n",
      "Epoch 3/5, Validation Loss: 7.4913\n",
      "hola amigo torch.Size([8, 197, 768])\n",
      "Epoch 4/5, Training Loss: 38.0103\n",
      "hola amigo torch.Size([1, 197, 768])\n",
      "Epoch 4/5, Validation Loss: 7.3608\n",
      "hola amigo torch.Size([8, 197, 768])\n",
      "Epoch 5/5, Training Loss: 39.9284\n",
      "hola amigo torch.Size([1, 197, 768])\n",
      "Epoch 5/5, Validation Loss: 7.2132\n"
     ]
    }
   ],
   "source": [
    "# Define the data paths and questions/answers\n",
    "\n",
    "image_dir = \"../segmentedImage\"\n",
    "json_path = \"../dataset/combined_data.json\"#\"dataset/combined_data.json\"\n",
    "\n",
    "\n",
    "questionId_path =\"../dataset/questionId_image_mapping.json\" #\"dataset/questionId_image_mapping.json\"\n",
    "\n",
    "# image_paths = [\"/home/dpadalia_umass_edu/685proj/pink_bear.jpg\"]\n",
    "# questions = [\"What is the color of the object?\"]\n",
    "# answers = [\"pink\"]\n",
    "\n",
    "# Load and preprocess the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Tokenizer and model initialization\n",
    "pretrained_model = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = load_and_split_dataset(image_dir,questionId_path, json_path, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=qa_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=qa_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=qa_collate_fn)\n",
    "\n",
    "# Load the ViT base model\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Initialize the QA-ViT model\n",
    "fusion_layers = 4\n",
    "qavit_model = QAViT(vit_model)\n",
    "\n",
    "# Load the T5 model for conditional generation\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n",
    "\n",
    "# Move models to the appropriate device (e.g., GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qavit_model.to(device)\n",
    "t5_model.to(device)\n",
    "\n",
    "# Train the QA-ViT model\n",
    "num_epochs = 5\n",
    "# train_qavit(qavit_model, t5_model, train_loader, tokenizer, num_epochs, device)\n",
    "train_qavit_with_validation(qavit_model, t5_model, train_loader, val_loader, tokenizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "def infer_qavit(qavit_model, t5_model, image_path, question, tokenizer, device):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize the question\n",
    "    question_tokens = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    question_tokens = {k: v.to(device) for k, v in question_tokens.items()}\n",
    "\n",
    "    # Perform inference with the QA-ViT model\n",
    "    qavit_model.eval()\n",
    "    with torch.no_grad():\n",
    "        visual_outputs = qavit_model(image)\n",
    "        \n",
    "        visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "    #print(\"visual features\",visual_features.shape)\n",
    "       \n",
    "\n",
    "        \n",
    "    # Construct the encoder outputs\n",
    "    encoder_hidden_state = visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1)\n",
    "    encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_state)\n",
    "\n",
    "    # Generate output with T5 model\n",
    "    t5_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ids = t5_model.generate(\n",
    "            input_ids=question_tokens[\"input_ids\"],\n",
    "            attention_mask=question_tokens[\"attention_mask\"],\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=10,\n",
    "            num_beams=1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).split()[0]\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola amigo torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/vqa/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: Subscribe\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/src/segmentedImage/262148_262148000_image.png\"\n",
    "question = \"Where is he looking?\"\n",
    "\n",
    "output_text = infer_qavit(qavit_model, t5_model, image_path, question, tokenizer, device)\n",
    "print(\"Generated Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/src/scripts/segmentedImage/262148_262148000_image.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./segmentedImage/262148_262148000_image.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere is he looking?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m output_text \u001b[38;5;241m=\u001b[39m \u001b[43minfer_qavit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqavit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt5_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_text)\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36minfer_qavit\u001b[0;34m(qavit_model, t5_model, image_path, question, tokenizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer_qavit\u001b[39m(qavit_model, t5_model, image_path, question, tokenizer, device):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load and preprocess the image\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m      8\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      9\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     10\u001b[0m     ])\n\u001b[1;32m     11\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/vqa/lib/python3.11/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/src/scripts/segmentedImage/262148_262148000_image.png'"
     ]
    }
   ],
   "source": [
    "image_path = \"./segmentedImage/262148_262148000_image.png\"\n",
    "question = \"Where is he looking?\"\n",
    "\n",
    "output_text = infer_qavit(qavit_model, t5_model, image_path, question, tokenizer, device)\n",
    "print(\"Generated Output:\", output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
