{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sudhanshu/anaconda3/lib/python311.zip', '/Users/sudhanshu/anaconda3/lib/python3.11', '/Users/sudhanshu/anaconda3/lib/python3.11/lib-dynload', '', '/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/vqa/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/VITLLMs/vqa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3588.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from /Users/sudhanshu/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "from models import NERModel, SamModelPrediction,NERRModel\n",
    "from segmentation import create_segmented_image\n",
    "import json\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from /Users/sudhanshu/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "ner = NERRModel(\"../lang-segment-anything/fineTunedT5ForNER\")\n",
    "#ner=NERModel()\n",
    "sam_predictor = SamModelPrediction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities using Transformer model\n",
    "# def extract_entities(question):\n",
    "#     results = ner.extract_entities(question)\n",
    "#     return [(res['word'], res['entity']) for res in results]\n",
    "\n",
    "\n",
    "def extract_entities(sentence):\n",
    "    instruction = f\" Extract exact words from the sentence that are place, animal, thing or location\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    1. Sentence: \"The cat sat on the mat?\" Common Nouns: cat, sat, mat.\n",
    "    2. Sentence: \"He drinks a lot of coffee and reads many books.\" Common Nouns:  coffee, reads, books.\n",
    "    3. Sentence: \"Sky is blue and grass is green.\" Common Nouns: sky, blue, grass, green.\n",
    "    4. Sentence: \"Is the  Boy is skateboarding on the wall?\" Common Nouns: boy, skateboarding, wall .\n",
    "    5. Sentence: \"What color is teddy bear \" Common Nouns: teddy bear, color\n",
    "    6. Sentence: \"Teddy bear is pink ?\" Common Nouns: teddy bear, pink\n",
    "    Sentence: \"{sentence}\"\n",
    "    Common Nouns:\"\"\"\n",
    "    # Encode the prompt\n",
    "    \n",
    "    \n",
    "    # Generate output\n",
    "    outputs = ner.extract_entities(instruction,prompt)\n",
    "\n",
    "    if ',' in outputs:\n",
    "        # Split the string at each comma and strip whitespace\n",
    "        entity_list = [entity.strip() for entity in outputs.split(',')]\n",
    "    else:\n",
    "        # Return the entire string as a single element list, also strip any whitespace\n",
    "        entity_list = [outputs.strip()]\n",
    "    return entity_list\n",
    "\n",
    " # Function to display image and entities\n",
    "\n",
    "def display_image_with_sam(image_id, entities, model):\n",
    "    base_url = \"http://images.cocodataset.org/val2014\"\n",
    "    image_url = f\"{base_url}/COCO_val2014_{image_id:012d}.jpg\"\n",
    "    \n",
    "    # Fetch the image\n",
    "    response = requests.get(image_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    \n",
    "    # Use SAM model to get segmentation masks for each entity\n",
    "    #prompts = [entity[0] for entity in entities]  # Assuming you want to segment based on the full entity text\n",
    "   \n",
    "    masks = [model.predict_image(image, ent) for ent in entities]\n",
    "    \n",
    "    masks = [item for sublist in masks for item in sublist]\n",
    "   \n",
    "    segmentedImageWithEntities = create_segmented_image(image,masks)\n",
    "  \n",
    "    # Display the original image and masks\n",
    "\n",
    "\n",
    "    return segmentedImageWithEntities\n",
    "\n",
    "def create_unique_filename(image_id):\n",
    "    # Create a unique filename using the image ID and the current timestamp\n",
    "   \n",
    "    return f\"{image_id}_image.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_entities(questions):\n",
    "    # Modify this function to handle batch processing.\n",
    "    # This is a placeholder. You'll need to implement batch processing in your actual entity extraction model.\n",
    "    return [extract_entities(question) for question in questions]\n",
    "\n",
    "def process_image(data):\n",
    "    try:\n",
    "        question, image_id, question_id, entities = data\n",
    "        directory = \"src/segmentedImage\"\n",
    "        name = f\"{image_id}_{question_id}\"\n",
    "        unique_filename = create_unique_filename(name)\n",
    "        segmented_image = display_image_with_sam(image_id, entities, sam_predictor)\n",
    "        segmented_image = segmented_image.convert('RGB')\n",
    "        segmented_image.save(os.path.join(directory, unique_filename))\n",
    "        return unique_filename, question, question_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['where is he looking?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background', 'people']\n",
      "['he']\n",
      "['website']\n",
      "['soup']\n",
      "['rice noodle soup']\n",
      "['right', 'soup']\n",
      "['street', 'thing']\n",
      "['photos']\n",
      "['truck', 'sell']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "file_path = '../dataset/combined_data.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "directory = \"../segmentedImage\"\n",
    "# Extract questions and image IDs\n",
    "questions_and_ids_question_id = [(item['question'], item['image_id'],item['question_id']) for item in data['annotations']]\n",
    "\n",
    "# # Load Transformer NER model\n",
    "questionsSegmentedImageMapping ={}\n",
    "questions_idSegmentedImageMapping ={}\n",
    "\n",
    "# all_questions = [item[0] for item in questions_and_ids_question_id]\n",
    "# entities= batch_extract_entities(all_questions)\n",
    "# data_to_process = [(q, iid, qid, entities) for (q, iid, qid), entities in zip(questions_and_ids_question_id, entities)]\n",
    "\n",
    "\n",
    "# Store results in dictionaries\n",
    "\n",
    "\n",
    "for question, image_id,question_id in questions_and_ids_question_id:\n",
    "    # Extract entities from the question\n",
    "    entities = extract_entities(question)\n",
    "    print(entities)\n",
    "    entities =  [item[0] for item in entities]\n",
    "    #entities=[\"people\",\"background\"]\n",
    "    # for entity in entities:\n",
    "       \n",
    "    #     if question.lower() in entity.lower():\n",
    "           \n",
    "          \n",
    "    #         entities=[]\n",
    "    \n",
    "    \n",
    "   \n",
    "  \n",
    "   \n",
    "    \n",
    "    # Generate a segmented image based on the image ID and entities\n",
    "    # This function should handle the creation and saving of the image\n",
    "\n",
    "    name= str(image_id)+\"_\"+ str(question_id)\n",
    "    unique_filename = create_unique_filename(name)\n",
    "    segmented_image = display_image_with_sam(image_id, entities, sam_predictor)\n",
    "    segmented_image.convert('RGB')\n",
    "    segmented_image.save(os.path.join(directory, unique_filename))\n",
    "    # # Save the segmented image with a unique filename\n",
    "\n",
    "    \n",
    "    # # Map the unique image filename to the question\n",
    "    questionsSegmentedImageMapping[unique_filename] = question\n",
    "    questions_idSegmentedImageMapping[unique_filename] =question_id\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background people']\n"
     ]
    }
   ],
   "source": [
    "question= \"What are background people doing ?\"\n",
    "ex= extract_entities(question)\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/question_image_mapping.json', 'w') as f:\n",
    "    json.dump(questionsSegmentedImageMapping, f)\n",
    "\n",
    "with open('../dataset/questionId_image_mapping.json', 'w') as f:\n",
    "    json.dump(questions_idSegmentedImageMapping, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
