{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5Tokenizer, T5EncoderModel, ViTModel, T5ForConditionalGeneration\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import get_cosine_schedule_with_warmup, AdamW\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoding(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(QuestionEncoding, self).__init__()\n",
    "        self.encoder = T5EncoderModel.from_pretrained(pretrained_model)\n",
    "        self.hidden_dim = self.encoder.config.hidden_size\n",
    "        self.projection_layers = nn.ModuleList([nn.Linear(self.hidden_dim, self.hidden_dim) for _ in range(self.encoder.config.num_layers)])\n",
    "\n",
    "    def forward(self, question):\n",
    "        input_ids = question[\"input_ids\"]\n",
    "        attention_mask = question[\"attention_mask\"]\n",
    "        encoded_question = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        projected_question = [projection(encoded_question) for projection in self.projection_layers]\n",
    "        return projected_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionFusing(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(QuestionFusing, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8)\n",
    "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.gating_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, visual_features, question_features):\n",
    "        batch_size, _ = visual_features.size()\n",
    "        visual_features = visual_features.unsqueeze(1)\n",
    "        fused_features = torch.cat((visual_features, question_features), dim=1)\n",
    "        attention_output, _ = self.attention(fused_features, fused_features, fused_features)\n",
    "        visual_output = attention_output[:, 0, :].unsqueeze(1)\n",
    "        projected_output = self.projection(visual_output)\n",
    "        gated_output = self.gating_projection(visual_output) * torch.tanh(self.beta)\n",
    "        fused_output = projected_output + gated_output\n",
    "        fused_output = fused_output.squeeze(1)\n",
    "        return fused_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAViT(nn.Module):\n",
    "    def __init__(self, vision_model, pretrained_model, fusion_layers):\n",
    "        super(QAViT, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.question_encoding = QuestionEncoding(pretrained_model)\n",
    "        self.question_fusing = nn.ModuleList([QuestionFusing(self.question_encoding.hidden_dim) for _ in range(fusion_layers)])\n",
    "        self.fusion_layers = fusion_layers\n",
    "\n",
    "    def forward(self, image, question):\n",
    "        visual_outputs = self.vision_model(pixel_values=image)\n",
    "        visual_features = visual_outputs.last_hidden_state\n",
    "        print(\"hola amigo before fusion\",visual_features.shape)\n",
    "        question_features = self.question_encoding(question)\n",
    "\n",
    "        num_layers = visual_features.shape[1]\n",
    "        start_layer = num_layers - self.fusion_layers\n",
    "\n",
    "        for i in range(start_layer, num_layers):\n",
    "            visual_features[:, i, :] = self.question_fusing[i - start_layer](visual_features[:, i, :], question_features[i - start_layer])\n",
    "        print(\"hola amigo\",visual_features.shape)\n",
    "        visual_outputs.last_hidden_state = visual_features\n",
    "        return visual_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training dataset\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations, tokenizer, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = annotations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        image_id = annotation[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        question = annotation[\"question\"]\n",
    "        answer = annotation[\"multiple_choice_answer\"]\n",
    "\n",
    "        question_tokens = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"question_tokens\": {k: v.squeeze(0) for k, v in question_tokens.items()},\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_collate_fn(batch):\n",
    "    # Stack images\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "\n",
    "    # Prepare questions\n",
    "    question_tokens = {k: torch.cat([item[\"question_tokens\"][k] for item in batch]) for k in batch[0][\"question_tokens\"]}\n",
    "    padded_question_tokens = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence([item[\"question_tokens\"][\"input_ids\"] for item in batch], batch_first=True, padding_value=0),\n",
    "        \"attention_mask\": torch.nn.utils.rnn.pad_sequence([item[\"question_tokens\"][\"attention_mask\"] for item in batch], batch_first=True, padding_value=0),\n",
    "    }\n",
    "\n",
    "    # Tokenize and pad answers\n",
    "    answers = [item[\"answer\"] for item in batch]\n",
    "    answer_tokens = tokenizer(answers, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        \"question_tokens\": padded_question_tokens,\n",
    "        \"answer\": answer_tokens\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(image_dir, json_path, tokenizer, transform):\n",
    "    # Load the annotations from JSON\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    annotations = data[\"annotations\"]\n",
    "\n",
    "    # Split into train (80%), val (10%), and test (10%)\n",
    "    train_annotations, test_annotations = train_test_split(annotations, test_size=0.2, random_state=42)\n",
    "    val_annotations, test_annotations = train_test_split(test_annotations, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Create Dataset objects\n",
    "    train_dataset = QADataset(image_dir, train_annotations, tokenizer, transform)\n",
    "    val_dataset = QADataset(image_dir, val_annotations, tokenizer, transform)\n",
    "    test_dataset = QADataset(image_dir, test_annotations, tokenizer, transform)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRa to T5 model\n",
    "def apply_lora_to_t5(t5_model):\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q\", \"k\"]\n",
    "    )\n",
    "    return get_peft_model(t5_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qavit_with_validation(qavit_model, t5_model, train_loader, val_loader, tokenizer, num_epochs, device):\n",
    "    qavit_model.train()\n",
    "    t5_model.train()\n",
    "\n",
    "    # Apply LoRa to T5 model\n",
    "    t5_model = apply_lora_to_t5(t5_model)\n",
    "\n",
    "    # Optimizer and Scheduler\n",
    "    optimizer = AdamW([\n",
    "        {\"params\": qavit_model.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": t5_model.parameters(), \"lr\": 5e-5}\n",
    "    ])\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=1000,\n",
    "        num_training_steps=len(train_loader) * num_epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        total_loss = 0\n",
    "        qavit_model.train()\n",
    "        t5_model.train()\n",
    "        for batch in train_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            question_tokens = {k: v.to(device) for k, v in batch[\"question_tokens\"].items()}\n",
    "            answers = {k: v.to(device) for k, v in batch[\"answer\"].items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through QA-ViT model\n",
    "            visual_outputs = qavit_model(images, question_tokens)\n",
    "            visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # Forward pass through T5 model\n",
    "            encoder_outputs = (visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1), None, None)\n",
    "\n",
    "            t5_outputs = t5_model(\n",
    "                input_ids=question_tokens[\"input_ids\"],\n",
    "                attention_mask=question_tokens[\"attention_mask\"],\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                labels=answers[\"input_ids\"]\n",
    "            )\n",
    "\n",
    "            loss = t5_outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        qavit_model.eval()\n",
    "        t5_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                question_tokens = {k: v.to(device) for k, v in batch[\"question_tokens\"].items()}\n",
    "                answers = {k: v.to(device) for k, v in batch[\"answer\"].items()}\n",
    "\n",
    "                # Forward pass through QA-ViT model\n",
    "                visual_outputs = qavit_model(images, question_tokens)\n",
    "                visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "                # Forward pass through T5 model\n",
    "                encoder_outputs = (visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1), None, None)\n",
    "\n",
    "                t5_outputs = t5_model(\n",
    "                    input_ids=question_tokens[\"input_ids\"],\n",
    "                    attention_mask=question_tokens[\"attention_mask\"],\n",
    "                    encoder_outputs=encoder_outputs,\n",
    "                    labels=answers[\"input_ids\"]\n",
    "                )\n",
    "\n",
    "                loss = t5_outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/Project/vqa/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the data paths and questions/answers\n",
    "image_dir = \"./segmentedImage\"\n",
    "json_path = \"/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/Project/dataset/combined_data.json\"#\"dataset/combined_data.json\"\n",
    "\n",
    "\n",
    "questionId_path =\"/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/Project/dataset/questionId_image_mapping.json\" #\"dataset/questionId_image_mapping.json\"\n",
    "\n",
    "\n",
    "# image_paths = [\"/home/dpadalia_umass_edu/685proj/pink_bear.jpg\"]\n",
    "# questions = [\"What is the color of the object?\"]\n",
    "# answers = [\"pink\"]\n",
    "\n",
    "# Load and preprocess the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Tokenizer and model initialization\n",
    "pretrained_model = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset, val_dataset, test_dataset = load_and_split_dataset(image_dir, json_path, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=qa_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=qa_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=qa_collate_fn)\n",
    "\n",
    "# Load the ViT base model\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Initialize the QA-ViT model\n",
    "fusion_layers = 4\n",
    "qavit_model = QAViT(vit_model, pretrained_model, fusion_layers)\n",
    "\n",
    "# Load the T5 model for conditional generation\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n",
    "\n",
    "# Move models to the appropriate device (e.g., GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qavit_model.to(device)\n",
    "t5_model.to(device)\n",
    "\n",
    "# Train the QA-ViT model\n",
    "num_epochs = 5\n",
    "# train_qavit(qavit_model, t5_model, train_loader, tokenizer, num_epochs, device)\n",
    "#train_qavit_with_validation(qavit_model, t5_model, train_loader, val_loader, tokenizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "def infer_qavit(qavit_model, t5_model, image_path, question, tokenizer, device):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize the question\n",
    "    question_tokens = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    question_tokens = {k: v.to(device) for k, v in question_tokens.items()}\n",
    "\n",
    "    # Perform inference with the QA-ViT model\n",
    "    qavit_model.eval()\n",
    "    with torch.no_grad():\n",
    "        visual_outputs = qavit_model(image, question_tokens)\n",
    "        print(\"visual features without mean \",visual_outputs.last_hidden_state.shape)\n",
    "        visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "    print(\"visual features\",visual_features.shape)\n",
    "        \n",
    "\n",
    "    # Construct the encoder outputs\n",
    "    encoder_hidden_state = visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1)\n",
    "    print(encoder_hidden_state.shape)\n",
    "    encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_state)\n",
    "\n",
    "    # Generate output with T5 model\n",
    "    t5_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ids = t5_model.generate(\n",
    "            input_ids=question_tokens[\"input_ids\"],\n",
    "            attention_mask=question_tokens[\"attention_mask\"],\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=10,\n",
    "            num_beams=1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).split()[0]\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola amigo before fusion torch.Size([1, 197, 768])\n",
      "hola amigo torch.Size([1, 197, 768])\n",
      "visual features without mean  torch.Size([1, 197, 768])\n",
      "visual features torch.Size([1, 768])\n",
      "torch.Size([1, 7, 768])\n",
      "Generated Output: Subscribe\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_path = \"/Users/sudhanshu/Desktop/UMASS_COURSES_SEMESTERS/SEM_2/NLP/Project/lang_segment_anything/segmentedImage/262148_262148000_image.png\"\n",
    "question = \"Where is he looking?\"\n",
    "\n",
    "output_text = infer_qavit(qavit_model, t5_model, image_path, question, tokenizer, device)\n",
    "print(\"Generated Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_qavit(image_path, question, pretrained_model, fusion_layers):\n",
    "#     # Load and preprocess the image\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "#     image = transform(image).unsqueeze(0)\n",
    "\n",
    "#     # Tokenize the question\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(pretrained_model)\n",
    "#     question_tokens = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "#     # Load the ViT base model\n",
    "#     vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "#     # Initialize the QAViT model\n",
    "#     qavit_model = QAViT(vit_model, pretrained_model, fusion_layers)\n",
    "\n",
    "#     # Move the image, question, and QAViT model to the appropriate device (e.g., GPU)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     # device = torch.device(\"cpu\")\n",
    "    \n",
    "#     image = image.to(device)\n",
    "#     question_tokens = {k: v.to(device) for k, v in question_tokens.items()}\n",
    "#     qavit_model.to(device)\n",
    "\n",
    "#     # Forward pass through the QAViT model\n",
    "#     with torch.no_grad():\n",
    "#         visual_outputs = qavit_model(image, question_tokens)\n",
    "#         visual_features = visual_outputs.last_hidden_state\n",
    "\n",
    "#     # Load the T5 model for conditional generation\n",
    "#     t5_model = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n",
    "#     t5_model.to(device)\n",
    "    \n",
    "#     input_ids = question_tokens[\"input_ids\"]\n",
    "#     attention_mask = question_tokens[\"attention_mask\"]\n",
    "#     visual_features = visual_features.mean(dim=1)  # Perform average pooling\n",
    "#     encoder_inputs = input_ids\n",
    "#     encoder_attention_mask = attention_mask\n",
    "\n",
    "#     output_ids = t5_model.generate(\n",
    "#         input_ids=encoder_inputs,\n",
    "#         attention_mask=encoder_attention_mask,\n",
    "#         max_length=100,\n",
    "#         num_beams=4,\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "\n",
    "#     # Decode the generated output\n",
    "#     output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#     return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# image_path = \"/home/dpadalia_umass_edu/685proj/pink_bear.jpg\"\n",
    "# question = \"What is the color of the object?\"\n",
    "# pretrained_model = \"google/flan-t5-base\"\n",
    "# fusion_layers = 4\n",
    "\n",
    "# output_text = run_qavit(image_path, question, pretrained_model, fusion_layers)\n",
    "# print(\"Generated Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training function\n",
    "# def train_qavit(qavit_model, t5_model, train_loader, tokenizer, num_epochs, device):\n",
    "#     qavit_model.train()\n",
    "#     t5_model.train()\n",
    "\n",
    "#     # Apply LoRa to T5 model\n",
    "#     t5_model = apply_lora_to_t5(t5_model)\n",
    "\n",
    "#     # Optimizer and Scheduler\n",
    "#     optimizer = AdamW([\n",
    "#         {\"params\": qavit_model.parameters(), \"lr\": 1e-4},\n",
    "#         {\"params\": t5_model.parameters(), \"lr\": 5e-5}\n",
    "#     ])\n",
    "#     scheduler = get_cosine_schedule_with_warmup(\n",
    "#         optimizer,\n",
    "#         num_warmup_steps=1000,\n",
    "#         num_training_steps=len(train_loader) * num_epochs\n",
    "#     )\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0\n",
    "#         for batch in train_loader:\n",
    "#             images = batch[\"image\"].to(device)\n",
    "#             question_tokens = {k: v.to(device) for k, v in batch[\"question_tokens\"].items()}\n",
    "#             answers = tokenizer(batch[\"answer\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#             answers = {k: v.to(device) for k, v in answers.items()}\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass through QA-ViT model\n",
    "#             visual_outputs = qavit_model(images, question_tokens)\n",
    "#             visual_features = visual_outputs.last_hidden_state.mean(dim=1)\n",
    "            \n",
    "#             encoder_outputs = (visual_features.unsqueeze(1).repeat(1, question_tokens[\"input_ids\"].size(1), 1), None, None)\n",
    "            \n",
    "#             # Forward pass through T5 model\n",
    "#             t5_outputs = t5_model(\n",
    "#                 input_ids=question_tokens[\"input_ids\"],\n",
    "#                 attention_mask=question_tokens[\"attention_mask\"],\n",
    "#                 encoder_outputs=encoder_outputs,\n",
    "#                 labels=answers[\"input_ids\"]\n",
    "#             )\n",
    "\n",
    "#             loss = t5_outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
